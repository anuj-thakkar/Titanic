{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part 3: Algorithm Selection\n",
    "Now that we have finished appropriately feature engineering dataset, we are ready to begin testing out different algorithms to see which is the most performant. This notebook will include the following actions:\n",
    "\n",
    "- Importing the cleansed dataset from the \"/data/clean\" directory\n",
    "- Splitting the cleansed dataset in two for a training and validation dataset\n",
    "- Performing a feature scaling where required\n",
    "- Performing a GridSearch for ideal hyperparameter tuning\n",
    "- Testing out a number of different algorithms\n",
    "- Validating the results of each algorithm with model validation metrics / visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the necessary Python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Importing the binary classification algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Loading in the cleaned dataset\n",
    "df_clean = pd.read_csv('../data/clean/new_data.csv')\n",
    "\n",
    "# Splitting the predictor value from the remainder of the dataset\n",
    "X = df_clean.drop(columns = ['Survived'])\n",
    "y = df_clean[['Survived']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a reusable function for churning through all five binary classification algorithms\n",
    "def generate_binary_classification_model(X, y, model_algorithm, hyperparameters, needs_scaled = False):\n",
    "    \"\"\"\n",
    "    Generating everything required for training and validation of a binary classification model\n",
    "\n",
    "    Args:\n",
    "        - X (Pandas DataFrame): A DataFrame containing the cleaned training data\n",
    "        - y (Pandas DataFrame): A DataFrame containing the target values correlated to the X training data\n",
    "        - model_algorithm (object): A model algorithm that will be trained against the X and y data\n",
    "        - hyperparameters (dict): A dictionary containing all the hyperparameters to test the model with\n",
    "        - needs_scaled (Boolean): A boolean value that indicates whether or not the input dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Performing a scaling on the data if required\n",
    "    if needs_scaled:\n",
    "\n",
    "        # Instantiating the StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        # Performing a fit_transform on the dataset\n",
    "        scaled_features = scaler.fit_transform(X)\n",
    "\n",
    "        # Transforming the StandardScaler output back into a Pandas DataFrame\n",
    "        X = pd.DataFrame(scaled_features, index = X.index, columns = X.columns)\n",
    "\n",
    "    # Instantiating a GridSearch object with the inputted model algorithm and hyperparameters\n",
    "    gridsearchcv = GridSearchCV(estimator = model_algorithm,\n",
    "                                param_grid = hyperparameters)\n",
    "\n",
    "    # Fitting the training data to the GridSearch object\n",
    "    gridsearchcv.fit(X, y)\n",
    "\n",
    "    # Printing out the best hyperparameters\n",
    "    print(f'Best hyperparameters: {gridsearchcv.best_params_}')\n",
    "\n",
    "    # Instantiating a new model object with the ideal hyperparameters from the GridSearch job\n",
    "    model_algorithm.set_params(**gridsearchcv.best_params_)\n",
    "\n",
    "    # Creating a container to hold each set of validation metrics\n",
    "    accuracy_scores, roc_auc_scores, f1_scores = [], [], []\n",
    "\n",
    "    # Instantiating the K-Fold cross validation object\n",
    "    k_fold = KFold(n_splits = 5)\n",
    "\n",
    "    # Iterating through each of the folds in K-Fold\n",
    "    for train_index, val_index in k_fold.split(X):\n",
    "\n",
    "        # Splitting the training set from the validation set for this specific fold\n",
    "        X_train, X_val = X.iloc[train_index, :], X.iloc[val_index, :]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        # Fitting the X_train and y_train datasets to the model algorithm\n",
    "        model_algorithm.fit(X_train, y_train)\n",
    "\n",
    "        # Getting inferential predictions for the validation dataset\n",
    "        val_preds = model_algorithm.predict(X_val)\n",
    "\n",
    "        # Generating validation metrics by comparing the inferential predictions (val_preds) to the actuals (y_val)\n",
    "        val_accuracy = accuracy_score(y_val, val_preds)\n",
    "        val_roc_auc_score = roc_auc_score(y_val, val_preds)\n",
    "        val_f1_score = f1_score(y_val, val_preds)\n",
    "\n",
    "        # Appending the validation scores to the respective validation metric container\n",
    "        accuracy_scores.append(val_accuracy)\n",
    "        roc_auc_scores.append(val_roc_auc_score)\n",
    "        f1_scores.append(val_f1_score)\n",
    "\n",
    "    # Getting the average (mean) of each validation score\n",
    "    average_accuracy = int(mean(accuracy_scores) * 100)\n",
    "    average_roc_auc_score = int(mean(roc_auc_scores) * 100)\n",
    "    average_f1_score = int(mean(f1_scores) * 100)\n",
    "\n",
    "    # Printing out the average validation metrics\n",
    "    print(f'Average accuracy score: {average_accuracy}%')\n",
    "    print(f'Average ROC AUC score: {average_roc_auc_score}%')\n",
    "    print(f'Average F1 score: {average_f1_score}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Some things to note:\n",
    "1. Feature Scaling: all data points need to be on similar scale. An algorithm like SVM may require it.\n",
    "2. Hyperparameter: parameter from a prior distribution; it captures the prior belief before data is observed.\n",
    "   - controls the behaviour of the training algorithm -> impact on the performance of the model is being trained\n",
    "\n",
    "Score outputs:\n",
    "1. Accuracy: __\n",
    "2. ROC AUC: __\n",
    "3. F1: __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'C': 10000.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Average accuracy score: 79%\n",
      "Average ROC AUC score: 78%\n",
      "Average F1 score: 72%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ALGORITHM 1: LOGISTIC REGRESSION\n",
    "this algorithm is used to predict the likelihood of all kinds of “yes” or “no” outcomes\n",
    "\"\"\"\n",
    "\n",
    "# Setting the hyperparameter grid for the Logistic Regression algorithm\n",
    "logistic_reg_params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': np.logspace(-4, 4, 20),\n",
    "    'solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "# Instantiating the Logistic Regression algorithm object\n",
    "logistic_reg_algorithm = LogisticRegression()\n",
    "\n",
    "# Feeding the algorithm into the reusable binary classification function\n",
    "generate_binary_classification_model(X = X, y = y, model_algorithm = logistic_reg_algorithm, hyperparameters = logistic_reg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'var_smoothing': 1e-06}\n",
      "Average accuracy score: 79%\n",
      "Average ROC AUC score: 78%\n",
      "Average F1 score: 73%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ALGORITHM 2: GAUSSIAN NAIVE BAEYS\n",
    "\n",
    "The Naïve Bayes classifier assumes that the value of one feature is independent of the value of any other feature. (Bayes Theorem)\n",
    "Naïve Bayes classifiers need training data to estimate the parameters required for classification\n",
    "\"\"\"\n",
    "\n",
    "# Setting the hyperparameter grid for the GaussianNB algorithm\n",
    "gaussian_nb_params = {\n",
    "    'var_smoothing': np.logspace(0, -9, num=100)\n",
    "}\n",
    "\n",
    "# Instantiating the GaussianNB algorithm object\n",
    "gaussian_nb_algorithm = GaussianNB()\n",
    "\n",
    "# Feeding the algorithm into the reusable binary classification function\n",
    "generate_binary_classification_model(X=X, y=y, model_algorithm=gaussian_nb_algorithm,\n",
    "                                     hyperparameters=gaussian_nb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'C': 1000, 'gamma': 1}\n",
      "Average accuracy score: 79%\n",
      "Average ROC AUC score: 76%\n",
      "Average F1 score: 70%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ALGORITHM 3: Support Vector Machine (Support Vector Classifier)\n",
    "\n",
    "Let’s imagine we have two tags: red and blue, and our data has two features: x and y.\n",
    "\n",
    "A support vector machine takes these data points and outputs the hyperplane\n",
    "(which in two dimensions it’s simply a line) that best separates the tags.\n",
    "This line is the decision boundary: anything that falls to one side of it we will classify as blue,\n",
    "and anything that falls to the other as red.\n",
    "\n",
    "This algorithm requires scaling.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Setting the hyperparameter grid for the Support Vector Machine (SVM) algorithm\n",
    "svm_params = {\n",
    "    'C': [0.1, 1000],\n",
    "    'gamma': [1, 0.0001],\n",
    "}\n",
    "# Instantiating the Support Vector Classifier (SVC) algorithm object\n",
    "svc_algorithm = SVC()\n",
    "\n",
    "# Feeding the algorithm into the reusable binary classification function\n",
    "generate_binary_classification_model(X=X, y=y, model_algorithm=svc_algorithm, hyperparameters=svm_params,\n",
    "                                     needs_scaled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 25}\n",
      "Average accuracy score: 83%\n",
      "Average ROC AUC score: 80%\n",
      "Average F1 score: 76%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "ALGORITHM 4: RANDOM FOREST CLASSIFIER\n",
    "\n",
    "- multiple decision trees\n",
    "- Assuming your dataset has “m” features, the random forest will randomly choose “k” features where k < m.\n",
    "- Now, the algorithm will calculate the root node among the k features by picking a node that has the highest information gain\n",
    "- After that, the algorithm splits the node into child nodes and repeats this process “n” times\n",
    "\n",
    "- GOAL: fits a number of decision tree classifiers on various sub-samples of the\n",
    "- dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Setting the hyperparameter grid for the Random Forest Classifier (RFC) algorithm\n",
    "rfc_params = {\n",
    "    'n_estimators': [25, 50, 75],\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'min_samples_split': [5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Instantiating the Random Forest Classifier (RFC) algorithm object\n",
    "rfc_algorithm = RandomForestClassifier()\n",
    "\n",
    "# Feeding the algorithm into the reusable binary classification function\n",
    "generate_binary_classification_model(X=X, y=y, model_algorithm=rfc_algorithm, hyperparameters=rfc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'depth': 2, 'iterations': 1, 'learning_rate': 0.001}\n",
      "Average accuracy score: 79%\n",
      "Average ROC AUC score: 77%\n",
      "Average F1 score: 71%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ALGORITHM 5: CATBOOST CLASSIFIER\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Setting the hyperparameter grid for the CatBoost Classifier algorithm\n",
    "catboost_params = {\n",
    "    'depth': [1, 2, 3],\n",
    "    'learning_rate': [0.001, 0.002, 0.003],\n",
    "    'iterations': [1, 2, 5]\n",
    "}\n",
    "# Instantiating the CatBoost Classifier algorithm object\n",
    "catboost_algorithm = CatBoostClassifier(silent=True)\n",
    "# Feeding the algorithm into the reusable binary classification function\n",
    "generate_binary_classification_model(X=X,\n",
    "                                     y=y,\n",
    "                                     model_algorithm=catboost_algorithm,\n",
    "                                     hyperparameters=catboost_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
