{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Part 4: Model Training Pipeline\n",
    "In the previous notebook, we settled on a model algorithm after validating it properly and are now ready to formalize the training pipeline from start to finish. The training pipeline will take the raw dataset as input and perform both the feature engineering and model training as a single pipeline. We will specifically do the following actions:\n",
    "\n",
    "- Importing the raw dataset from the \"/data/raw\" directory\n",
    "- Splitting the data into training and validation datasets\n",
    "- Using our feature engineering and model algorithm code to build an end-to-end training pipeline\n",
    "- Saving the model as a serialized pickle file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The algorithm to be used will be Random Forest Classifier:\n",
    "1. Best hyperparameters: {'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 75}\n",
    "2. Average accuracy score: 82%\n",
    "3. Average ROC AUC score: 81%"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the necessary Python libraries\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Hiding any warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Adjusting Pandas output\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loading in the training data\n",
    "df_raw = pd.read_csv('../data/raw/titanic-train-raw.csv')\n",
    "\n",
    "# Separating predictor value from the remainder of the dataset\n",
    "X = df_raw.drop(columns = ['Survived'])\n",
    "y = df_raw[['Survived']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}